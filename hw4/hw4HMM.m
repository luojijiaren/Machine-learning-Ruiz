% hw4 HMM test
%I
clc;clear;close

TRANS = [0 0.5 0.2 0.3; 0 0.4 0.3 0.3; 0 0.2 0.6 0.2;  0 0.1 0.1 0.8];
EMIS = [0; 1; 1; 1];
STATES=[];
rng(1)
for i=1:100,
    [seq,states] = hmmgenerate(1000,TRANS,EMIS,'Statenames',{'start','S1','S2','S3'});
    STATES=[STATES;states];
end
STATES;
    
%II
SEQ=ones(size(STATES));
TR_sum=zeros(size(TRANS));EM_sum=zeros(size(EMIS));
for i=1:100,
    seq=SEQ(i,:);states=STATES(i,:);
    [TR_temp,EM_temp] = hmmestimate(seq,states,'Statenames',{'start','S1','S2','S3'})
    TR_sum=TR_sum+TR_temp;
    EM_sum=EM_sum+EM_temp;
end
TR=TR_sum/100
EM=EM_sum/100


%III
clc;clear;close

%Consider the Coke/Pepsi hidden Markov Model (HMM) used in Prof. Ruiz's example of Viterbi's, Forward, and Backward algorithms.
%Two emission, 'Coke'=c; 'Pepsi"=P.
%Hidden state: 
%start: fake start state,1
%A: The price of Coke and Pepsi are the same,2
%R: “Red sale”: Coke is on sale (cheaper than Pepsi),3
%B: “Blue sale”: Pepsi is on sale (cheaper than Coke),4

%Q1
%the probability that this sequence was generated by our HMM
C = 1;P = 2;
%seqen0=[C P C]
seqen = [P P P P C C P P P C C C P C C C C C P P P C P C P];
TRANS = [0 0.6 0.1 0.3; 0 0.2 0.1 0.7; 0 0.1 0.1 0.8;  0 0.4 0.3 0.3];
EMIS = [0 0; 0.6 0.4; 0.9 0.1; 0.5 0.5];
[PSTATES,logpseq] = hmmdecode(seqen,TRANS,EMIS);
p=exp(logpseq)
%Q2
%the most likely sequence of hidden states that generated this sequence
likelystates = hmmviterbi(seqen, TRANS, EMIS,'Statenames',{'start','A','R','B'})

%Q3
%(10 points) Assume that the sequence is numbered stating at 1 
%(i.e., the first element of the sequence, "P", is at position 1). 
%What is the most likely hidden state that generated the "C" in position 10 of the sequence? Explain.
seqen1 = [P P P P C C P P P C]
seqen2 = [C C P C C C C C P P P C P C P];
TRANS = [0 0.6 0.1 0.3; 0 0.2 0.1 0.7; 0 0.1 0.1 0.8;  0 0.4 0.3 0.3];
EMIS = [0 0; 0.6 0.4; 0.9 0.1; 0.5 0.5];
[PSTATES1,logpseq1] = hmmdecode(seqen1,TRANS,EMIS);
[PSTATES2,logpseq2] = hmmdecode(seqen2,TRANS,EMIS);
ptotal=(exp(logpseq1)*exp(logpseq2))/p

%Use the HMM to generate a sequence of observables of length 2000. 
%Then, use this generated sequence to learn the transition probabilities and the emision probabilities
rng(2)
[seq,states] = hmmgenerate(2000,TRANS,EMIS,'Symbols',{'C','P'},'Statenames',{'start','A','R','B'});
[TRANS,EMIS] = hmmestimate(seq,states,'Symbols',{'C','P'},'Statenames',{'A','R','B'})